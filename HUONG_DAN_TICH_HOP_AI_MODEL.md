# H∆∞·ªõng D·∫´n T√≠ch H·ª£p M√¥ H√¨nh AI V·ªõi Frontend tr√™n Vercel

## üìã T·ªïng Quan

B·∫°n c√≥:

- **Backend Python**: YOLO model + Multi-Agent System (orchestrator)
- **Frontend Next.js**: ƒêang d√πng Vercel AI Gateway v·ªõi xAI models
- **M·ª•c ti√™u**: K·∫øt n·ªëi Python backend v·ªõi Next.js frontend tr√™n Vercel

## üéØ C√≥ 3 C√°ch Ti·∫øp C·∫≠n

### C√°ch 1: Python API Server (Khuy·∫øn ngh·ªã cho Production)

- Deploy Python backend ri√™ng (Railway, Render, AWS Lambda, etc.)
- Next.js g·ªçi API qua HTTP
- ‚úÖ T√°ch bi·ªát, d·ªÖ scale
- ‚úÖ C√≥ th·ªÉ d√πng GPU cho YOLO

### C√°ch 2: Vercel Serverless Functions (Python)

- Deploy Python code tr·ª±c ti·∫øp tr√™n Vercel
- ‚úÖ T·∫•t c·∫£ trong m·ªôt project
- ‚ö†Ô∏è Gi·ªõi h·∫°n v·ªÅ th·ªùi gian ch·∫°y v√† dependencies

### C√°ch 3: Custom AI Provider trong Next.js

- T·∫°o custom provider g·ªçi Python backend
- ‚úÖ T√≠ch h·ª£p s√¢u v·ªõi AI SDK
- ‚úÖ Streaming support

---

## üöÄ C√°ch 1: Python API Server (Khuy·∫øn ngh·ªã)

### B∆∞·ªõc 1: T·∫°o Python API Server

T·∫°o file `api_server.py` trong th∆∞ m·ª•c g·ªëc:

```python
"""
FastAPI server ƒë·ªÉ expose Python AI models
"""
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn
import asyncio
from typing import Optional
import base64
from io import BytesIO
from PIL import Image

from orchestrator import AgentOrchestrator

app = FastAPI(title="Plant Disease AI API")

# CORS middleware ƒë·ªÉ cho ph√©p frontend g·ªçi
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Trong production, ch·ªâ ƒë·ªãnh domain c·ª• th·ªÉ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Kh·ªüi t·∫°o orchestrator
orchestrator = AgentOrchestrator()

@app.get("/")
async def root():
    return {"message": "Plant Disease AI API", "status": "running"}

@app.get("/health")
async def health():
    return {"status": "healthy"}

@app.post("/api/chat")
async def chat_endpoint(
    user_query: str,
    user_context: Optional[dict] = None,
    image_data: Optional[str] = None,  # base64 encoded image
    image_file: Optional[UploadFile] = None,
):
    """
    Endpoint ch√≠nh ƒë·ªÉ x·ª≠ l√Ω chat v·ªõi AI

    Args:
        user_query: C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
        user_context: Context b·ªï sung (plant_type, location, etc.)
        image_data: Base64 encoded image (n·∫øu c√≥)
        image_file: Uploaded image file (n·∫øu c√≥)
    """
    try:
        # X·ª≠ l√Ω image n·∫øu c√≥
        image_path = None
        if image_file:
            # L∆∞u file t·∫°m
            import tempfile
            import os
            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp:
                tmp.write(await image_file.read())
                image_path = tmp.name
        elif image_data:
            # Decode base64 v√† l∆∞u
            import tempfile
            image_bytes = base64.b64decode(image_data)
            with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp:
                tmp.write(image_bytes)
                image_path = tmp.name

        # Chu·∫©n b·ªã input
        user_input = {
            "user_query": user_query,
            "user_context": user_context or {},
        }

        if image_path:
            user_input["image_path"] = image_path

        # G·ªçi orchestrator
        result = await orchestrator.execute(user_input)

        # Cleanup
        if image_path and os.path.exists(image_path):
            os.unlink(image_path)

        return JSONResponse({
            "status": "success",
            "result": result,
        })

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/detect")
async def detect_disease(
    image_file: UploadFile,
    conf_threshold: float = 0.25,
):
    """
    Endpoint ri√™ng cho YOLO detection
    """
    try:
        from yolo.inference_yolo import YOLOInference

        # L∆∞u file t·∫°m
        import tempfile
        import os
        with tempfile.NamedTemporaryFile(delete=False, suffix='.jpg') as tmp:
            tmp.write(await image_file.read())
            image_path = tmp.name

        # Ch·∫°y YOLO
        yolo = YOLOInference("models/yolo_detection_s.pt", conf_threshold=conf_threshold)
        results = yolo.predict_single(image_path)

        # Cleanup
        if os.path.exists(image_path):
            os.unlink(image_path)

        return JSONResponse({
            "status": "success",
            "detections": results,
        })

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### B∆∞·ªõc 2: T·∫°o requirements cho API server

T·∫°o file `requirements-api.txt`:

```txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6
pydantic==2.5.0
```

### B∆∞·ªõc 3: T·∫°o Next.js API Route ƒë·ªÉ g·ªçi Python backend

T·∫°o file `FE/ai-chatbot-main/app/(chat)/api/plant-ai/route.ts`:

```typescript
import { NextRequest, NextResponse } from 'next/server';

const PYTHON_API_URL = process.env.PYTHON_API_URL || 'http://localhost:8000';

export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { user_query, user_context, image_data } = body;

    // G·ªçi Python API
    const response = await fetch(`${PYTHON_API_URL}/api/chat`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        user_query,
        user_context,
        image_data, // base64 encoded
      }),
    });

    if (!response.ok) {
      throw new Error(`Python API error: ${response.statusText}`);
    }

    const data = await response.json();

    return NextResponse.json({
      status: 'success',
      result: data.result,
    });
  } catch (error: any) {
    console.error('Error calling Python API:', error);
    return NextResponse.json({ error: error.message }, { status: 500 });
  }
}

export async function GET() {
  // Health check
  try {
    const response = await fetch(`${PYTHON_API_URL}/health`);
    const data = await response.json();
    return NextResponse.json(data);
  } catch (error: any) {
    return NextResponse.json({ status: 'unhealthy', error: error.message }, { status: 503 });
  }
}
```

### B∆∞·ªõc 4: T·∫°o Custom Tool trong AI SDK

T·∫°o file `FE/ai-chatbot-main/lib/ai/tools/plant-diagnosis.ts`:

```typescript
import { tool } from 'ai';
import { z } from 'zod';

const PYTHON_API_URL = process.env.PYTHON_API_URL || 'http://localhost:8000';

export const plantDiagnosis = tool({
  description: `
    Ch·∫©n ƒëo√°n b·ªánh c√¢y tr·ªìng d·ª±a tr√™n:
    - C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng v·ªÅ tri·ªáu ch·ª©ng
    - H√¨nh ·∫£nh c√¢y b·ªã b·ªánh (n·∫øu c√≥)
    - Context v·ªÅ lo·∫°i c√¢y, v·ªã tr√≠, m√πa v·ª•

    S·ª≠ d·ª•ng YOLO model v√† multi-agent system ƒë·ªÉ ƒë∆∞a ra:
    - Ch·∫©n ƒëo√°n b·ªánh
    - Khuy·∫øn ngh·ªã ƒëi·ªÅu tr·ªã
    - Bi·ªán ph√°p ph√≤ng ng·ª´a
  `,
  parameters: z.object({
    user_query: z.string().describe('C√¢u h·ªèi ho·∫∑c m√¥ t·∫£ v·ªÅ v·∫•n ƒë·ªÅ c√¢y tr·ªìng'),
    plant_type: z.string().optional().describe('Lo·∫°i c√¢y (v√≠ d·ª•: c√† chua, l√∫a, v.v.)'),
    location: z.string().optional().describe('V·ªã tr√≠ (v√≠ d·ª•: mi·ªÅn B·∫Øc, mi·ªÅn Nam)'),
    season: z.string().optional().describe('M√πa v·ª• (v√≠ d·ª•: m√πa m∆∞a, m√πa kh√¥)'),
    image_url: z.string().url().optional().describe('URL c·ªßa h√¨nh ·∫£nh c√¢y b·ªã b·ªánh'),
  }),
  execute: async ({ user_query, plant_type, location, season, image_url }) => {
    try {
      // N·∫øu c√≥ image_url, fetch v√† convert sang base64
      let image_data: string | undefined;
      if (image_url) {
        const imageResponse = await fetch(image_url);
        const imageBuffer = await imageResponse.arrayBuffer();
        image_data = Buffer.from(imageBuffer).toString('base64');
      }

      const response = await fetch(`${PYTHON_API_URL}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          user_query,
          user_context: {
            plant_type,
            location,
            season,
          },
          image_data,
        }),
      });

      if (!response.ok) {
        throw new Error(`Python API error: ${response.statusText}`);
      }

      const data = await response.json();
      const result = data.result;

      // Format response cho AI
      const finalAdvice = result.final_advice || {};

      return {
        diagnosis: finalAdvice.diagnosis || 'Kh√¥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c b·ªánh',
        confidence: finalAdvice.confidence_score || 0,
        recommendations: finalAdvice.recommendations || [],
        treatment: finalAdvice.treatment_plan || 'Ch∆∞a c√≥ k·∫ø ho·∫°ch ƒëi·ªÅu tr·ªã',
        prevention: finalAdvice.prevention_measures || [],
        full_advice: finalAdvice.full_advice || finalAdvice.summary || '',
      };
    } catch (error: any) {
      return {
        error: error.message,
        diagnosis: 'L·ªói khi ch·∫©n ƒëo√°n',
      };
    }
  },
});
```

### B∆∞·ªõc 5: Th√™m Tool v√†o Chat Route

C·∫≠p nh·∫≠t `FE/ai-chatbot-main/app/(chat)/api/chat/route.ts`:

```typescript
// Th√™m import
import { plantDiagnosis } from '@/lib/ai/tools/plant-diagnosis';

// Trong streamText, th√™m tool:
tools: {
  getWeather,
  createDocument: createDocument({ session, dataStream }),
  updateDocument: updateDocument({ session, dataStream }),
  requestSuggestions: requestSuggestions({
    session,
    dataStream,
  }),
  plantDiagnosis, // ‚Üê Th√™m d√≤ng n√†y
},
```

### B∆∞·ªõc 6: Th√™m Environment Variable

Th√™m v√†o `.env.local`:

```env
# Python API URL
PYTHON_API_URL=http://localhost:8000  # Local development
# PYTHON_API_URL=https://your-python-api.railway.app  # Production
```

---

## üöÄ C√°ch 2: Vercel Serverless Functions (Python)

### B∆∞·ªõc 1: T·∫°o Vercel Function

T·∫°o file `FE/ai-chatbot-main/api/plant-ai.py`:

```python
from http.server import BaseHTTPRequestHandler
import json
import sys
import os

# Add parent directory to path ƒë·ªÉ import orchestrator
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))

from orchestrator import AgentOrchestrator

class handler(BaseHTTPRequestHandler):
    def do_POST(self):
        try:
            content_length = int(self.headers['Content-Length'])
            post_data = self.rfile.read(content_length)
            data = json.loads(post_data.decode('utf-8'))

            orchestrator = AgentOrchestrator()
            result = await orchestrator.execute({
                "user_query": data.get("user_query", ""),
                "user_context": data.get("user_context", {}),
            })

            self.send_response(200)
            self.send_header('Content-type', 'application/json')
            self.send_header('Access-Control-Allow-Origin', '*')
            self.end_headers()
            self.wfile.write(json.dumps(result).encode())

        except Exception as e:
            self.send_response(500)
            self.send_header('Content-type', 'application/json')
            self.end_headers()
            self.wfile.write(json.dumps({"error": str(e)}).encode())
```

### B∆∞·ªõc 2: C·∫•u h√¨nh Vercel

T·∫°o file `vercel.json`:

```json
{
  "functions": {
    "api/plant-ai.py": {
      "runtime": "python3.9"
    }
  }
}
```

‚ö†Ô∏è **L∆∞u √Ω**: Vercel Python functions c√≥ gi·ªõi h·∫°n v·ªÅ:

- Th·ªùi gian ch·∫°y (10s cho Hobby, 60s cho Pro)
- Dependencies size
- Kh√¥ng h·ªó tr·ª£ GPU

---

## üöÄ C√°ch 3: Custom AI Provider

T·∫°o custom provider ƒë·ªÉ t√≠ch h·ª£p s√¢u h∆°n v·ªõi AI SDK:

T·∫°o file `FE/ai-chatbot-main/lib/ai/providers-custom.ts`:

```typescript
import { customProvider, languageModel } from 'ai';

const PYTHON_API_URL = process.env.PYTHON_API_URL || 'http://localhost:8000';

// Custom language model wrapper
export const plantDiseaseModel = languageModel({
  provider: 'custom',
  modelId: 'plant-disease-ai',
  doStream: async ({ prompt, messages }) => {
    // G·ªçi Python API
    const response = await fetch(`${PYTHON_API_URL}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        user_query: messages[messages.length - 1]?.content || prompt,
      }),
    });

    const data = await response.json();
    const advice = data.result?.final_advice?.full_advice || '';

    // Return streaming response
    return {
      stream: (async function* () {
        // Simulate streaming
        const words = advice.split(' ');
        for (const word of words) {
          yield { type: 'text-delta', textDelta: word + ' ' };
        }
      })(),
      rawCall: { rawPrompt: prompt, rawSettings: {} },
    };
  },
});

export const customPlantProvider = customProvider({
  languageModels: {
    'plant-disease-model': plantDiseaseModel,
  },
});
```

---

## üì¶ Deploy Python Backend

### Option A: Railway (Khuy·∫øn ngh·ªã)

1. T·∫°o account t·∫°i [Railway.app](https://railway.app)
2. T·∫°o new project t·ª´ GitHub repo
3. Set build command: `pip install -r requirements-api.txt`
4. Set start command: `python api_server.py`
5. Add environment variables
6. Deploy!

### Option B: Render

1. T·∫°o account t·∫°i [Render.com](https://render.com)
2. T·∫°o new Web Service
3. Connect GitHub repo
4. Set:
   - Build: `pip install -r requirements-api.txt`
   - Start: `uvicorn api_server:app --host 0.0.0.0 --port $PORT`
5. Deploy!

### Option C: AWS Lambda + API Gateway

- Ph·ª©c t·∫°p h∆°n nh∆∞ng scalable
- C·∫ßn setup Lambda layers cho dependencies

---

## üîß C·∫•u H√¨nh Frontend

### 1. Th√™m Model v√†o Model Selector

C·∫≠p nh·∫≠t `FE/ai-chatbot-main/lib/ai/models.ts`:

```typescript
export const chatModels = [
  // ... existing models
  {
    id: 'plant-disease-model',
    name: 'Plant Disease AI',
    description: 'Ch·∫©n ƒëo√°n b·ªánh c√¢y tr·ªìng v·ªõi YOLO + Multi-Agent',
  },
] as const;
```

### 2. Update Providers

C·∫≠p nh·∫≠t `FE/ai-chatbot-main/lib/ai/providers.ts`:

```typescript
import { customPlantProvider } from './providers-custom';

export const myProvider = isTestEnvironment
  ? // ... existing
  : customProvider({
      languageModels: {
        // ... existing models
        'plant-disease-model': customPlantProvider.languageModel('plant-disease-model'),
      },
    });
```

---

## üß™ Testing

### Test Python API locally:

```bash
# Terminal 1: Start Python API
cd D:\TTNT2
python api_server.py

# Terminal 2: Test API
curl -X POST http://localhost:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{"user_query": "C√¢y c√† chua b·ªã v√†ng l√°", "user_context": {"plant_type": "c√† chua"}}'
```

### Test t·ª´ Frontend:

1. Start Next.js: `npm run dev`
2. M·ªü browser: http://localhost:3000
3. Ch·ªçn model "Plant Disease AI"
4. G·ª≠i message: "C√¢y c√† chua c·ªßa t√¥i b·ªã v√†ng l√°"

---

## üìù Environment Variables

### Frontend (.env.local):

```env
PYTHON_API_URL=http://localhost:8000  # Development
# PYTHON_API_URL=https://your-api.railway.app  # Production
```

### Python Backend:

```env
OPENAI_API_KEY=your-key-here
# Other env vars...
```

---

## üéØ Next Steps

1. ‚úÖ T·∫°o Python API server
2. ‚úÖ Deploy Python backend (Railway/Render)
3. ‚úÖ T·∫°o Next.js API route
4. ‚úÖ T·∫°o custom tool/provider
5. ‚úÖ Test integration
6. ‚úÖ Deploy frontend l√™n Vercel
7. ‚úÖ Set production PYTHON_API_URL

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [Vercel Serverless Functions](https://vercel.com/docs/functions)
- [AI SDK Custom Providers](https://ai-sdk.dev/docs/guides/providers/custom-provider)
- [Railway Deployment](https://docs.railway.app/)
- [Render Deployment](https://render.com/docs)
