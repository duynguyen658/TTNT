"""
Script training YOLOv8 Detection model nh·∫≠n d·∫°ng b·ªánh c√¢y tr·ªìng
H·ªó tr·ª£ dataset YOLO format v·ªõi bounding boxes annotations
"""
import os
import sys
import yaml
import json
import shutil
from pathlib import Path
from typing import Optional, List, Dict
import random

# Ki·ªÉm tra YOLO
try:
    from ultralytics import YOLO
    YOLO_AVAILABLE = True
    print(f"‚úÖ Ultralytics YOLO ƒë√£ s·∫µn s√†ng")
except ImportError as e:
    YOLO_AVAILABLE = False
    print(f"‚ùå Ultralytics YOLO ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t: {e}")
    print("üëâ Gi·∫£i ph√°p: pip install ultralytics")
    sys.exit(1)


def validate_yolo_dataset(dataset_path: str, data_yaml_path: Optional[str] = None):
    """
    Ki·ªÉm tra v√† validate dataset YOLO detection format
    
    YOLO Detection format:
    dataset/
        train/
            images/
                img1.jpg
                img2.jpg
            labels/
                img1.txt
                img2.txt
        val/ (ho·∫∑c valid/)
            images/
                img1.jpg
            labels/
                img1.txt
        test/ (optional)
            images/
                img1.jpg
            labels/
                img1.txt
        data.yaml
    """
    dataset_path = Path(dataset_path)
    
    if not dataset_path.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y dataset t·∫°i: {dataset_path}")
        return False, None, None
    
    # T√¨m file data.yaml
    if data_yaml_path:
        yaml_path = Path(data_yaml_path)
    else:
        yaml_path = dataset_path / "data.yaml"
        if not yaml_path.exists():
            # Th·ª≠ t√¨m trong th∆∞ m·ª•c cha
            yaml_path = dataset_path.parent / "data.yaml"
    
    if not yaml_path.exists():
        print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y data.yaml, s·∫Ω t·∫°o t·ª± ƒë·ªông")
        return True, None, None
    
    # ƒê·ªçc data.yaml
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f)
        
        class_names = data_config.get('names', [])
        num_classes = data_config.get('nc', len(class_names))
        
        print(f"\nüìã Dataset config:")
        print(f"   S·ªë classes: {num_classes}")
        print(f"   Classes: {class_names[:5]}{'...' if len(class_names) > 5 else ''}")
        
        # Ki·ªÉm tra c·∫•u tr√∫c th∆∞ m·ª•c
        train_path = Path(data_config.get('train', 'train'))
        val_path = Path(data_config.get('val', 'valid'))
        
        # N·∫øu l√† relative path, resolve t·ª´ dataset_path
        if not train_path.is_absolute():
            train_path = dataset_path.parent / train_path if '..' in str(train_path) else dataset_path / train_path
        if not val_path.is_absolute():
            val_path = dataset_path.parent / val_path if '..' in str(val_path) else dataset_path / val_path
        
        # Ki·ªÉm tra images v√† labels
        train_images = train_path / "images" if (train_path / "images").exists() else train_path
        train_labels = train_path / "labels" if (train_path / "labels").exists() else train_path
        
        val_images = val_path / "images" if (val_path / "images").exists() else val_path
        val_labels = val_path / "labels" if (val_path / "labels").exists() else val_path
        
        # ƒê·∫øm s·ªë ·∫£nh
        train_img_files = list(train_images.glob("*.jpg")) + list(train_images.glob("*.JPG")) + \
                         list(train_images.glob("*.png")) + list(train_images.glob("*.PNG"))
        val_img_files = list(val_images.glob("*.jpg")) + list(val_images.glob("*.JPG")) + \
                      list(val_images.glob("*.png")) + list(val_images.glob("*.PNG"))
        
        print(f"\nüìä Th·ªëng k√™ dataset:")
        print(f"   Train images: {len(train_img_files)}")
        print(f"   Val images: {len(val_img_files)}")
        
        # Ki·ªÉm tra labels
        train_label_files = list(train_labels.glob("*.txt"))
        val_label_files = list(val_labels.glob("*.txt"))
        
        print(f"   Train labels: {len(train_label_files)}")
        print(f"   Val labels: {len(val_label_files)}")
        
        if len(train_img_files) == 0:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y ·∫£nh training")
            return False, None, None
        
        if len(train_label_files) == 0:
            print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y labels, dataset c√≥ th·ªÉ ch·ªâ c√≥ ·∫£nh (classification)")
            return False, None, None
        
        return True, str(yaml_path), data_config
        
    except Exception as e:
        print(f"‚ùå L·ªói khi ƒë·ªçc data.yaml: {e}")
        return False, None, None


def create_data_yaml(
    dataset_path: str,
    class_names: List[str],
    train_path: str = "train/images",
    val_path: str = "valid/images",
    test_path: Optional[str] = None,
    output_path: Optional[str] = None
):
    """
    T·∫°o file data.yaml cho YOLO detection
    """
    dataset_path = Path(dataset_path)
    
    if output_path is None:
        output_path = dataset_path / "data.yaml"
    else:
        output_path = Path(output_path)
    
    data_config = {
        'path': str(dataset_path.absolute()),
        'train': train_path,
        'val': val_path,
        'nc': len(class_names),
        'names': class_names
    }
    
    if test_path:
        data_config['test'] = test_path
    
    # L∆∞u file
    with open(output_path, 'w', encoding='utf-8') as f:
        yaml.dump(data_config, f, default_flow_style=False, allow_unicode=True)
    
    print(f"‚úÖ ƒê√£ t·∫°o data.yaml t·∫°i: {output_path}")
    return str(output_path)


def train_yolo_detection(
    data_yaml_path: str,
    model_size: str = "s",  # n, s, m, l, x
    epochs: int = 100,
    imgsz: int = 640,
    batch: int = 16,
    device: Optional[str] = None,
    output_dir: str = "runs/detect",
    project_name: str = "plant_disease_detection",
    **kwargs
):
    """
    Train YOLOv8 Detection model
    
    Args:
        data_yaml_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file data.yaml
        model_size: K√≠ch th∆∞·ªõc model (n, s, m, l, x)
        epochs: S·ªë epochs
        imgsz: K√≠ch th∆∞·ªõc ·∫£nh (m·∫∑c ƒë·ªãnh 640 cho detection)
        batch: Batch size
        device: Device ('cpu', 'cuda', '0', '1', etc.)
        output_dir: Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£
        project_name: T√™n project
        **kwargs: C√°c tham s·ªë kh√°c cho model.train()
    """
    if not YOLO_AVAILABLE:
        print("‚ùå YOLO ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        return None
    
    print("=" * 60)
    print("TRAINING YOLOv8 DETECTION MODEL")
    print("=" * 60)
    print()
    
    # Ki·ªÉm tra data.yaml
    yaml_path = Path(data_yaml_path)
    if not yaml_path.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y data.yaml t·∫°i: {data_yaml_path}")
        return None
    
    # ƒê·ªçc v√† fix data.yaml n·∫øu c·∫ßn (convert relative paths to absolute)
    temp_yaml = None
    try:
        with open(yaml_path, 'r', encoding='utf-8') as f:
            data_config = yaml.safe_load(f)
        
        # Fix relative paths trong data.yaml
        yaml_dir = yaml_path.parent
        if 'path' not in data_config or not Path(data_config['path']).is_absolute():
            data_config['path'] = str(yaml_dir.absolute())
        
        # Fix train/val paths v√† validate
        for split in ['train', 'val', 'test']:
            if split in data_config:
                split_path = Path(data_config[split])
                if not split_path.is_absolute():
                    # N·∫øu l√† relative path, resolve t·ª´ yaml_dir
                    if '..' in str(split_path):
                        resolved_path = (yaml_dir.parent / split_path).resolve()
                    else:
                        resolved_path = (yaml_dir / split_path).resolve()
                    data_config[split] = str(resolved_path)
                
                # Ki·ªÉm tra labels folder
                split_path = Path(data_config[split])
                labels_path = split_path.parent / "labels" if split_path.name == "images" else split_path / "labels"
                
                if labels_path.exists():
                    label_files = list(labels_path.glob("*.txt"))
                    if len(label_files) == 0:
                        print(f"‚ö†Ô∏è  C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y file .txt trong {labels_path}")
                    else:
                        print(f"‚úÖ {split}: T√¨m th·∫•y {len(label_files)} label files")
                else:
                    print(f"‚ö†Ô∏è  C·∫£nh b√°o: Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c labels t·∫°i {labels_path}")
        
        # L∆∞u l·∫°i data.yaml ƒë√£ fix (t·∫°m th·ªùi)
        temp_yaml = yaml_path.with_suffix('.temp.yaml')
        with open(temp_yaml, 'w', encoding='utf-8') as f:
            yaml.dump(data_config, f, default_flow_style=False, allow_unicode=True)
        
        # S·ª≠ d·ª•ng temp yaml
        yaml_path = temp_yaml
        print(f"üìù ƒê√£ fix paths trong data.yaml")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ fix data.yaml: {e}, s·ª≠ d·ª•ng file g·ªëc")
    
    # Load pretrained model
    model_name = f"yolov8{model_size}.pt"
    print(f"üì• ƒêang t·∫£i pretrained model: {model_name}")
    
    try:
        model = YOLO(model_name)
        print(f"‚úÖ ƒê√£ t·∫£i model: {model_name}")
    except Exception as e:
        print(f"‚ùå Kh√¥ng th·ªÉ t·∫£i model: {e}")
        return None
    
    # Training parameters
    print(f"\n‚öôÔ∏è  C·∫•u h√¨nh training:")
    print(f"   Data config: {data_yaml_path}")
    print(f"   Model: {model_name}")
    print(f"   Epochs: {epochs}")
    print(f"   Image size: {imgsz}")
    print(f"   Batch size: {batch}")
    print(f"   Device: {device or 'auto'}")
    print()
    
    # Training parameters cho detection
    train_params = {
        "data": str(yaml_path.absolute()),
        "epochs": epochs,
        "imgsz": imgsz,
        "batch": batch,
        "device": device,
        "project": output_dir,
        "name": project_name,
        "exist_ok": True,
        "pretrained": True,
        "verbose": True,
        "seed": 42,
        "deterministic": True,
        "amp": True,  # Automatic Mixed Precision
        # Augmentation parameters
        "hsv_h": 0.015,
        "hsv_s": 0.7,
        "hsv_v": 0.4,
        "degrees": 0.0,
        "translate": 0.1,
        "scale": 0.5,
        "shear": 0.0,
        "perspective": 0.0,
        "flipud": 0.0,
        "fliplr": 0.5,
        "mosaic": 1.0,
        "mixup": 0.0,
        "copy_paste": 0.0,
        # Learning rate
        "lr0": 0.01,
        "lrf": 0.01,
        "momentum": 0.937,
        "weight_decay": 0.0005,
        "warmup_epochs": 3.0,
        "warmup_momentum": 0.8,
        "warmup_bias_lr": 0.1,
        # Loss weights
        "box": 7.5,
        "cls": 0.5,
        "dfl": 1.5,
        # Other
        "close_mosaic": 10,
        "resume": False,
        "fraction": 1.0,
        "profile": False,
        "freeze": None,
        # Windows fix: set workers=0 ƒë·ªÉ tr√°nh l·ªói multiprocessing/paging file
        "workers": 0 if os.name == 'nt' else 8,  # Windows: 0, Linux/Mac: 8
        **kwargs
    }
    
    # Train
    try:
        print("üöÄ B·∫Øt ƒë·∫ßu training...")
        results = model.train(**train_params)
        
        print("\n" + "=" * 60)
        print("‚úÖ TRAINING HO√ÄN TH√ÄNH!")
        print("=" * 60)
        
        # T√¨m file model t·ªët nh·∫•t
        best_model_path = Path(output_dir) / project_name / "weights" / "best.pt"
        last_model_path = Path(output_dir) / project_name / "weights" / "last.pt"
        
        if best_model_path.exists():
            print(f"\nüì¶ Model t·ªët nh·∫•t: {best_model_path}")
            print(f"üì¶ Model cu·ªëi c√πng: {last_model_path}")
            
            # Copy model v√†o th∆∞ m·ª•c models
            models_dir = Path("models")
            models_dir.mkdir(exist_ok=True)
            
            final_model_path = models_dir / f"yolo_detection_{model_size}.pt"
            shutil.copy2(best_model_path, final_model_path)
            print(f"üì¶ ƒê√£ copy model v√†o: {final_model_path}")
            
            # L∆∞u class names v√† config
            with open(yaml_path, 'r', encoding='utf-8') as f:
                data_config = yaml.safe_load(f)
            
            classes_file = final_model_path.with_suffix('.json').with_name(
                final_model_path.stem + '_config.json'
            )
            with open(classes_file, 'w', encoding='utf-8') as f:
                json.dump({
                    "class_names": data_config.get('names', []),
                    "num_classes": data_config.get('nc', 0),
                    "model_size": model_size,
                    "mode": "detection"
                }, f, indent=2, ensure_ascii=False)
            
            print(f"üìù ƒê√£ l∆∞u config: {classes_file}")
            
            # Hi·ªÉn th·ªã metrics n·∫øu c√≥
            if hasattr(results, 'results_dict'):
                print(f"\nüìä Metrics:")
                metrics = results.results_dict
                for key, value in metrics.items():
                    if isinstance(value, (int, float)):
                        print(f"   {key}: {value:.4f}")
        
        # Cleanup temp yaml file
        if temp_yaml and Path(temp_yaml).exists():
            try:
                Path(temp_yaml).unlink()
            except:
                pass
        
        return results
        
    except Exception as e:
        print(f"‚ùå L·ªói khi training: {e}")
        import traceback
        traceback.print_exc()
        
        # Cleanup temp yaml file
        if temp_yaml and Path(temp_yaml).exists():
            try:
                Path(temp_yaml).unlink()
            except:
                pass
        
        return None


def train_from_dataset_path(
    dataset_path: str = "data",
    data_yaml_path: Optional[str] = None,
    model_size: str = "s",
    epochs: int = 100,
    imgsz: int = 640,
    batch: int = 16,
    device: Optional[str] = None,
    output_dir: str = "runs/detect",
    project_name: str = "plant_disease_detection"
):
    """
    Train YOLO detection t·ª´ ƒë∆∞·ªùng d·∫´n dataset (t·ª± ƒë·ªông t√¨m data.yaml)
    """
    print("=" * 60)
    print("KI·ªÇM TRA DATASET")
    print("=" * 60)
    print()
    
    # Validate dataset
    is_valid, yaml_path, data_config = validate_yolo_dataset(dataset_path, data_yaml_path)
    
    if not is_valid:
        print("\n‚ùå Dataset kh√¥ng h·ª£p l·ªá ho·∫∑c kh√¥ng t√¨m th·∫•y")
        print("\nüëâ Dataset c·∫ßn c√≥ c·∫•u tr√∫c:")
        print("   dataset/")
        print("       train/")
        print("           images/")
        print("           labels/")
        print("       valid/")
        print("           images/")
        print("           labels/")
        print("       data.yaml")
        return None
    
    if yaml_path is None:
        print("\n‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y data.yaml, c·∫ßn t·∫°o th·ªß c√¥ng")
        return None
    
    # Train
    results = train_yolo_detection(
        data_yaml_path=yaml_path,
        model_size=model_size,
        epochs=epochs,
        imgsz=imgsz,
        batch=batch,
        device=device,
        output_dir=output_dir,
        project_name=project_name
    )
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Train YOLOv8 Detection model nh·∫≠n d·∫°ng b·ªánh c√¢y tr·ªìng',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
V√≠ d·ª• s·ª≠ d·ª•ng:

1. Train v·ªõi data.yaml m·∫∑c ƒë·ªãnh:
   python train_yolo_detection.py --data data/data.yaml --epochs 100

2. Train t·ª´ th∆∞ m·ª•c dataset (t·ª± ƒë·ªông t√¨m data.yaml):
   python train_yolo_detection.py --dataset data --epochs 100

3. Train v·ªõi model l·ªõn h∆°n:
   python train_yolo_detection.py --data data/data.yaml --model-size m --epochs 150

4. Train tr√™n GPU:
   python train_yolo_detection.py --data data/data.yaml --device cuda

5. Train v·ªõi batch size nh·ªè h∆°n (n·∫øu GPU memory kh√¥ng ƒë·ªß):
   python train_yolo_detection.py --data data/data.yaml --batch 8
        """
    )
    
    parser.add_argument('--data', '--data-yaml', type=str, default=None,
                       dest='data_yaml',
                       help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn file data.yaml')
    parser.add_argument('--dataset', type=str, default='data',
                       help='ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c dataset (t·ª± ƒë·ªông t√¨m data.yaml)')
    parser.add_argument('--model-size', type=str, default='s',
                       choices=['n', 's', 'm', 'l', 'x'],
                       help='K√≠ch th∆∞·ªõc model: n (nano), s (small), m (medium), l (large), x (xlarge)')
    parser.add_argument('--epochs', type=int, default=100,
                       help='S·ªë epochs ƒë·ªÉ train (m·∫∑c ƒë·ªãnh: 100)')
    parser.add_argument('--imgsz', type=int, default=640,
                       help='K√≠ch th∆∞·ªõc ·∫£nh (m·∫∑c ƒë·ªãnh: 640 cho detection)')
    parser.add_argument('--batch', type=int, default=16,
                       help='Batch size (m·∫∑c ƒë·ªãnh: 16)')
    parser.add_argument('--device', type=str, default=None,
                       help='Device (cpu, cuda, 0, 1, etc.). M·∫∑c ƒë·ªãnh: auto')
    parser.add_argument('--output-dir', type=str, default='runs/detect',
                       help='Th∆∞ m·ª•c l∆∞u k·∫øt qu·∫£ (m·∫∑c ƒë·ªãnh: runs/detect)')
    parser.add_argument('--project-name', type=str, default='plant_disease_detection',
                       help='T√™n project (m·∫∑c ƒë·ªãnh: plant_disease_detection)')
    
    args = parser.parse_args()
    
    # Train
    if args.data_yaml:
        # Train v·ªõi data.yaml ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
        results = train_yolo_detection(
            data_yaml_path=args.data_yaml,
            model_size=args.model_size,
            epochs=args.epochs,
            imgsz=args.imgsz,
            batch=args.batch,
            device=args.device,
            output_dir=args.output_dir,
            project_name=args.project_name
        )
    else:
        # Train t·ª´ dataset path (t·ª± ƒë·ªông t√¨m data.yaml)
        results = train_from_dataset_path(
            dataset_path=args.dataset,
            model_size=args.model_size,
            epochs=args.epochs,
            imgsz=args.imgsz,
            batch=args.batch,
            device=args.device,
            output_dir=args.output_dir,
            project_name=args.project_name
        )
